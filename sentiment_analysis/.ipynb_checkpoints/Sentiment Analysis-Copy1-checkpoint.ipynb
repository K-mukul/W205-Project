{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Sentiment Analysis\n",
    "\n",
    "Miki Seltzer\n",
    "Eric Whyne\n",
    "Jasen Jones\n",
    "\n",
    "To get this working you need to make sure nltk is installed with:\n",
    "\n",
    "####sudo pip install nltk\n",
    "####nltk.download()\n",
    "\n",
    "The download part opens a new dialogue box that allows you to download all the packages.\n",
    "\n",
    "We also need a couple other libraries\n",
    "\n",
    "####sudo pip install pyyaml\n",
    "\n",
    "That one is for importing dictionaries - these include our positive, negative, incrementing, decrementing, and inverting words.  We have to build those dictionaries ourselves, but this makes it easy to put those in a file (or to import existing ones).\n",
    "\n",
    "####sudo pip install pprintpp\n",
    "\n",
    "This library just makes it easier for us to see each token on it's own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    "import yaml\n",
    "import pprint\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If you have not downloaded the NLTK files, this will do it for you:\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##This is taken directly from http://fjavieralba.com/basic-sentiment-analysis-with-python.html\n",
    "##Two classes that split, tokenize, and tag.\n",
    "\n",
    "class Splitter(object):\n",
    "    '''Splits sentences into individual tokens'''\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    '''Assigns each token with a part of speech tag'''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This section calls the functions to split, tokenize, and tag the text.\n",
    "\n",
    "text = \"\"\"What can I say about this place. The staff of the restaurant is nice and the eggplant is not bad. Apart from that, very uninspired food, lack of atmosphere and too expensive. I am a staunch vegetarian and was sorely dissapointed with the veggie options on the menu. Will be the last time I visit, I recommend others to avoid.\"\"\"\n",
    "\n",
    "splitter = Splitter()\n",
    "postagger = POSTagger()\n",
    "\n",
    "splitted_sentences = splitter.split(text)\n",
    "\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next class (DictionaryTagger) tags a token with a sentiment based on dictionary values we store in various yaml dictionary files.  There are five types of sentiment we use, each affecting the final sentiment value:\n",
    "\n",
    "Positive: Our core positive words\n",
    "Examples: Great, good, best, etc.\n",
    "\n",
    "Negative: Our core negative words\n",
    "Examples: Awful, stupid, terrible, etc.\n",
    "\n",
    "Incrementers: Words that increase the strength of the next word\n",
    "Examples: Totally, extremely, absolutely, etc.\n",
    "\n",
    "Decrementers: Words that decrease the strength of the next word\n",
    "Examples: Kinda, sorta, etc.\n",
    "\n",
    "Inverters: Words that totally change the meaning of the next word\n",
    "Examples: Not, aren't, can't, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DictionaryTagger(object):\n",
    "\n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we feed the sentiment words to the dictionary tagger.  The dictionary terms for positive and negative sentiment were taken from:\n",
    "\n",
    "Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" \n",
    ";       Proceedings of the ACM SIGKDD International Conference on Knowledge \n",
    ";       Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, \n",
    ";       Washington, USA, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   [   ('What', 'What', ['WP']),\n",
      "        ('can', 'can', ['MD']),\n",
      "        ('I', 'I', ['PRP']),\n",
      "        ('say', 'say', ['VBP']),\n",
      "        ('about', 'about', ['IN']),\n",
      "        ('this', 'this', ['DT']),\n",
      "        ('place', 'place', ['NN']),\n",
      "        ('.', '.', ['.'])],\n",
      "    [   ('The', 'The', ['DT']),\n",
      "        ('staff', 'staff', ['NN']),\n",
      "        ('of', 'of', ['IN']),\n",
      "        ('the', 'the', ['DT']),\n",
      "        ('restaurant', 'restaurant', ['NN']),\n",
      "        ('is', 'is', ['VBZ']),\n",
      "        ('nice', 'nice', ['positive', 'JJ']),\n",
      "        ('and', 'and', ['CC']),\n",
      "        ('the', 'the', ['DT']),\n",
      "        ('eggplant', 'eggplant', ['NN']),\n",
      "        ('is', 'is', ['VBZ']),\n",
      "        ('not', 'not', ['inv', 'RB']),\n",
      "        ('bad', 'bad', ['negative', 'JJ']),\n",
      "        ('.', '.', ['.'])],\n",
      "    [   ('Apart', 'Apart', ['RB']),\n",
      "        ('from', 'from', ['IN']),\n",
      "        ('that', 'that', ['IN']),\n",
      "        (',', ',', [',']),\n",
      "        ('very', 'very', ['inc', 'RB']),\n",
      "        ('uninspired', 'uninspired', ['JJ']),\n",
      "        ('food', 'food', ['NN']),\n",
      "        (',', ',', [',']),\n",
      "        ('lack of', 'lack of', ['inv']),\n",
      "        ('atmosphere', 'atmosphere', ['NN']),\n",
      "        ('and', 'and', ['CC']),\n",
      "        ('too', 'too', ['inc', 'RB']),\n",
      "        ('expensive', 'expensive', ['negative', 'JJ']),\n",
      "        ('.', '.', ['.'])],\n",
      "    [   ('I', 'I', ['PRP']),\n",
      "        ('am', 'am', ['VBP']),\n",
      "        ('a', 'a', ['DT']),\n",
      "        ('staunch', 'staunch', ['positive', 'NN']),\n",
      "        ('vegetarian', 'vegetarian', ['NN']),\n",
      "        ('and', 'and', ['CC']),\n",
      "        ('was', 'was', ['VBD']),\n",
      "        ('sorely', 'sorely', ['negative', 'RB']),\n",
      "        ('dissapointed', 'dissapointed', ['negative', 'VBN']),\n",
      "        ('with', 'with', ['IN']),\n",
      "        ('the', 'the', ['DT']),\n",
      "        ('veggie', 'veggie', ['NN']),\n",
      "        ('options', 'options', ['NNS']),\n",
      "        ('on', 'on', ['IN']),\n",
      "        ('the', 'the', ['DT']),\n",
      "        ('menu', 'menu', ['NN']),\n",
      "        ('.', '.', ['.'])],\n",
      "    [   ('Will', 'Will', ['MD']),\n",
      "        ('be', 'be', ['VB']),\n",
      "        ('the', 'the', ['DT']),\n",
      "        ('last', 'last', ['JJ']),\n",
      "        ('time', 'time', ['NN']),\n",
      "        ('I', 'I', ['PRP']),\n",
      "        ('visit', 'visit', ['VBP']),\n",
      "        (',', ',', [',']),\n",
      "        ('I', 'I', ['PRP']),\n",
      "        ('recommend', 'recommend', ['positive', 'VBP']),\n",
      "        ('others', 'others', ['NNS']),\n",
      "        ('to', 'to', ['TO']),\n",
      "        ('avoid', 'avoid', ['VB']),\n",
      "        ('.', '.', ['.'])]]\n"
     ]
    }
   ],
   "source": [
    "dicttagger = DictionaryTagger([ 'positive.yml', 'negative.yml', 'increasers.yml', 'decreasers.yml', 'inverter.yml'])\n",
    "\n",
    "dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "\n",
    "#The following just lets us see what the split and tagged tokens look like.\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "pp.pprint(dict_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def sentence_score(sentence_tokens, previous_token, acum_score):    \n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentiment_score(review):\n",
    "    return sum([sentence_score(sentence, None, 0.0) for sentence in review])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section takes each tweet and throws it against the splitting, tagging, and scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an empty list to store the tweets and their sentiment values\n",
    "sentiments = []\n",
    "\n",
    "with open(\"tweet-sentiment_sample.log\", 'w') as outfile:\n",
    "\n",
    "    #Opens the file from whatever directory the iPython notebook was launched from. \n",
    "    #You'll need a different path if the dictionay files are in a separate folder.\n",
    "\n",
    "    filename = \"clean-tweets_sample.log\"\n",
    "\n",
    "    line_generator = open(filename)\n",
    "\n",
    "    for line in line_generator:\n",
    "        #Here we cycle through each tweet and apply all the tagging functions\n",
    "        line_object = json.loads(line)\n",
    "\n",
    "        #This requires a \"try\" call because some tweets apparently don't have text\n",
    "        try:\n",
    "            tweet = line_object['text']\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        #The workhorse - all of our splitting, tagging, and scoring\n",
    "        #We are now filtering for only tweets that have \"black friday\" or \"blackfriday\"\n",
    "        date = line_object['date']\n",
    "        splitted_sentences = splitter.split(tweet)\n",
    "        pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "        dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "        score = sentiment_score(dict_tagged_sentences)\n",
    "\n",
    "        #Places all the date, text and scores into a list for efficiency, then converts it to a numpy array for now\n",
    "        #sentiments.append([date, tweet, score])\n",
    "\n",
    "        #Immediately write all tweets with scores to JSON\n",
    "        data = {}\n",
    "        data['text'] = tweet\n",
    "        data['date'] = date\n",
    "        data['score'] = score\n",
    "        json.dump(data, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "#tweetandscore = np.asarray(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Originally we were writing to numpy, but numpy does not handle unicode well.\n",
    "#Thus, we no longer store all the scores and tweets in numpy\n",
    "\n",
    "#Now we can check the individual tweets and their respective scores in numpy.  \n",
    "#This should be done by appending to the database, but this method will work for now.  \n",
    "#0 is neutral, higher numbers are positive, and lower numbers are negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   u'contributors': None,\n",
      "    u'coordinates': None,\n",
      "    u'created_at': u'Tue Nov 17 05:03:43 +0000 2015',\n",
      "    u'entities': {   u'hashtags': [   {   u'indices': [91, 97],\n",
      "                                          u'text': u'verge'},\n",
      "                                      {   u'indices': [98, 103],\n",
      "                                          u'text': u'news'},\n",
      "                                      {   u'indices': [104, 111],\n",
      "                                          u'text': u'latest'}],\n",
      "                     u'symbols': [],\n",
      "                     u'urls': [   {   u'display_url': u'on.recode.net/1NY6NEB',\n",
      "                                      u'expanded_url': u'http://on.recode.net/1NY6NEB',\n",
      "                                      u'indices': [67, 90],\n",
      "                                      u'url': u'https://t.co/0zwVCpvOIc'}],\n",
      "                     u'user_mentions': []},\n",
      "    u'favorite_count': 0,\n",
      "    u'favorited': False,\n",
      "    u'filter_level': u'low',\n",
      "    u'geo': None,\n",
      "    u'id': 666481822319009793L,\n",
      "    u'id_str': u'666481822319009793',\n",
      "    u'in_reply_to_screen_name': None,\n",
      "    u'in_reply_to_status_id': None,\n",
      "    u'in_reply_to_status_id_str': None,\n",
      "    u'in_reply_to_user_id': None,\n",
      "    u'in_reply_to_user_id_str': None,\n",
      "    u'is_quote_status': False,\n",
      "    u'lang': u'en',\n",
      "    u'place': None,\n",
      "    u'possibly_sensitive': False,\n",
      "    u'retweet_count': 0,\n",
      "    u'retweeted': False,\n",
      "    u'source': u'<a href=\"http://twitterfeed.com\" rel=\"nofollow\">twitterfeed</a>',\n",
      "    u'text': u'With Rdio buy, Pandora envisions Apple Music model for new service https://t.co/0zwVCpvOIc #verge #news #latest',\n",
      "    u'timestamp_ms': u'1447736623731',\n",
      "    u'truncated': False,\n",
      "    u'user': {   u'contributors_enabled': False,\n",
      "                 u'created_at': u'Mon Feb 23 09:38:14 +0000 2015',\n",
      "                 u'default_profile': False,\n",
      "                 u'default_profile_image': False,\n",
      "                 u'description': u'Into all things #digital & #tech, privacy advocate, proud geek, LFC fan & marketeer. \\nDigital Media Strategist with @purevpn! \\n(Email: ali.mansoor@purevpn.com)',\n",
      "                 u'favourites_count': 20,\n",
      "                 u'follow_request_sent': None,\n",
      "                 u'followers_count': 140,\n",
      "                 u'following': None,\n",
      "                 u'friends_count': 747,\n",
      "                 u'geo_enabled': False,\n",
      "                 u'id': 3055323119L,\n",
      "                 u'id_str': u'3055323119',\n",
      "                 u'is_translator': False,\n",
      "                 u'lang': u'en',\n",
      "                 u'listed_count': 39,\n",
      "                 u'location': None,\n",
      "                 u'name': u'Ali Mansoor',\n",
      "                 u'notifications': None,\n",
      "                 u'profile_background_color': u'131516',\n",
      "                 u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme14/bg.gif',\n",
      "                 u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme14/bg.gif',\n",
      "                 u'profile_background_tile': True,\n",
      "                 u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/3055323119/1425373265',\n",
      "                 u'profile_image_url': u'http://pbs.twimg.com/profile_images/569793978468278272/7KfZvG9z_normal.jpeg',\n",
      "                 u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/569793978468278272/7KfZvG9z_normal.jpeg',\n",
      "                 u'profile_link_color': u'000000',\n",
      "                 u'profile_sidebar_border_color': u'C0DEED',\n",
      "                 u'profile_sidebar_fill_color': u'DDEEF6',\n",
      "                 u'profile_text_color': u'333333',\n",
      "                 u'profile_use_background_image': True,\n",
      "                 u'protected': False,\n",
      "                 u'screen_name': u'digitalmansoor',\n",
      "                 u'statuses_count': 12157,\n",
      "                 u'time_zone': u'Karachi',\n",
      "                 u'url': u'http://www.purevpn.com',\n",
      "                 u'utc_offset': 18000,\n",
      "                 u'verified': False}}\n"
     ]
    }
   ],
   "source": [
    "#Just so we can see what a single tweet looks like\n",
    "pp.pprint(line_object)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
